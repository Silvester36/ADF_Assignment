# Databricks notebook source
# Databricks notebook source
import re
from pyspark.sql import SparkSession, DataFrame
from pyspark.sql.functions import udf, col
from pyspark.sql.types import StringType


spark = SparkSession.builder.appName("CamelToSnake_Example").getOrCreate()


def camel_to_snake(name: str) -> str:
    
    s1 = re.sub(r'(.)([A-Z][a-z]+)', r'\1_\2', name)
    return re.sub(r'([a-z0-9])([A-Z])', r'\1_\2', s1).lower()

def convert_columns_to_snakecase(df: DataFrame) -> DataFrame:
    new_cols = [camel_to_snake(c) for c in df.columns]
    return df.toDF(*new_cols)

camel_to_snake_udf = udf(lambda x: camel_to_snake(x) if x is not None else None, StringType())
spark.udf.register("camel_to_snake_udf", camel_to_snake_udf)
df = spark.read.option("header", "true").csv("/Volumes/medallion_architecture/layers/bronze")
for c in df.columns: df = df.withColumn(c, camel_to_snake_udf(col(c)))

print(" Columns after rename:", df.columns)
display(df)


# COMMAND ----------

from pyspark.sql.functions import col, split

df_split2 = df.withColumn(
    "first_name", split(col("Name"), " ").getItem(0)
).withColumn(
    "last_name", split(col("Name"), " ").getItem(1)
)
display(df_split2)

df_with_domain = df_split2.withColumn(
    "domain",
    split(split(col("Email Id"), "@").getItem(1), "\.").getItem(0)
)
display(df_with_domain)

# COMMAND ----------

# COMMAND ----------

from pyspark.sql.functions import col, when, lower, trim

df4 = df_with_domain.withColumn(
    "gender",
    when(lower(trim(col("gender"))) == "female", "F")
    .when(lower(trim(col("gender"))) == "male", "M")
    .otherwise(col("gender"))
)

display(df4)




# COMMAND ----------

# COMMAND ----------

from pyspark.sql.functions import col, to_timestamp, to_date, split
 
df_dateAndTime = (
   df4
        # STEP 1: Convert entire string to a proper timestamp
        .withColumn("ts", to_timestamp(col("Joining Date"), "dd-MM-yyyy HH:mm"))
       
        # STEP 2: Extract date in yyyy-MM-dd format
        .withColumn("date", to_date(col("ts")))
       
        # STEP 3: Extract time as HH:mm:ss
        .withColumn("time", split(col("Joining Date"), " ").getItem(1))
 
        # drop extra column
        .drop("ts")
)
 
display(df_dateAndTime)


# COMMAND ----------

#Derive “expenditure_status” based on “spent” column
df_expenditureStatus = df4.withColumn(
    "expenditure_status",
    when(col("spent") < 500, "Minimum").otherwise("Maximum")
)
display(df_expenditureStatus)
 


# COMMAND ----------

def clean_column_names(df):
    new_cols = []
    for c in df.columns:
        clean = re.sub(r"[ ,;{}()\n\t=]", "_", c)
        clean = re.sub("_+", "_", clean)
        clean = clean.lower()
        new_cols.append(clean)
    return df.toDF(*new_cols)





# COMMAND ----------

from pyspark.sql.functions import *
from delta.tables import DeltaTable
import re

# 1. Correct clean column function
def clean_column_names(df):
    new_cols = []
    for c in df.columns:
        clean = re.sub(r"[ ,;{}()\n\t=]", "_", c)
        clean = re.sub("_+", "_", clean)
        clean = clean.lower()
        new_cols.append(clean)
    return df.toDF(*new_cols)

# 2. Use the final DF you want to store (IMPORTANT!)
df_final = df_expenditureStatus        # <--- change this to your last dataframe
df_clean = clean_column_names(df_final)

display(df_clean)

# 3. Deduplicate for merge
df_unique = df_clean.dropDuplicates(["customer_id"])

# 4. Silver path & table
silver_path = "/Volumes/medallion_architecture/layers/silver/customer"
table_name = "medallion_architecture.sales_view.customer"

# 5. Create schema if missing
spark.sql("""
    CREATE SCHEMA IF NOT EXISTS medallion_architecture.sales_view
""")

# 6. Merge or Create Table
from delta.tables import DeltaTable
 
if spark.catalog.tableExists(table_name):
    delta_table = DeltaTable.forName(spark, table_name)
   
    (delta_table.alias("target")
     .merge(
         df_unique.alias("source"),
         "target.customer_id = source.customer_id"
     )
     .whenMatchedUpdateAll()
     .whenNotMatchedInsertAll()
     .execute()
    )
 
else:
    (df_unique.write.format("delta")
      .mode("overwrite")
      .saveAsTable(table_name))
 
    print(" Product data successfully upserted into Silver layer Catalog.")

# COMMAND ----------

spark.read.format("delta").load(silver_path).display()

# COMMAND ----------

df=spark.read.csv("abfss://bronze@silva.dfs.core.windows.net/products", inferSchema=True, header=True)
display(df)

# COMMAND ----------

# MAGIC %md
# MAGIC https://silva.blob.core.windows.net/bronze/products/

# COMMAND ----------

import re
 
def rename_columns_to_snake_case(df):
    def camel_to_snake(colname):
        s1 = re.sub('(.)([A-Z][a-z]+)', r'\1_\2', colname)
        colname = re.sub('([a-z0-9])([A-Z])', r'\1_\2', s1)
        return colname.lower().replace(" ", "_")
   
    new_cols = [camel_to_snake(c) for c in df.columns]
    return df.toDF(*new_cols)
 
df_snake = rename_columns_to_snake_case(df)
df_snake.display()

# COMMAND ----------

from pyspark.sql.functions import when, col
 
df_create = df_snake.withColumn(
    "sub_category",
    when(col("category_id") == 1, "phone")
    .when(col("category_id") == 2, "laptop")
    .when(col("category_id") == 3, "playstation")
    .when(col("category_id") == 4, "e-device")
    .otherwise("unknown")
)
display(df_create)

# COMMAND ----------

df_dup = df_create.dropDuplicates(["product_id"])

# COMMAND ----------

from delta.tables import DeltaTable
 
table_name = "medallion_architecture.sales_view.product"
 
if spark.catalog.tableExists(table_name):
    delta_table = DeltaTable.forName(spark, table_name)
   
    (delta_table.alias("target")
     .merge(
         df_dup.alias("source"),
         "target.product_id = source.product_id"
     )
     .whenMatchedUpdateAll()
     .whenNotMatchedInsertAll()
     .execute()
    )
 
else:
    (df_dup.write.format("delta")
      .mode("overwrite")
      .saveAsTable(table_name))
 
    print(" Product data successfully upserted into Silver layer Catalog.")

# COMMAND ----------

df = spark.read.format("csv") \
    .option("header", "true") \
    .option("inferSchema", "true") \
    .load("abfs://bronze@silva.dfs.core.windows.net/store/")
display(df)

# COMMAND ----------

def rename_columns_to_snake_case(df):
    def camel_to_snake(colname):
        s1 = re.sub('(.)([A-Z][a-z]+)', r'\1_\2', colname)
        colname = re.sub('([a-z0-9])([A-Z])', r'\1_\2', s1)
        return colname.lower().replace(" ", "_")
   
    new_cols = [camel_to_snake(c) for c in df.columns]
    return df.toDF(*new_cols)
 
df_store = rename_columns_to_snake_case(df)
display(df_store)
 

# COMMAND ----------

from pyspark.sql.functions import regexp_extract, col

df_2 = df_store.withColumn(
    "store_category",
    regexp_extract(col("email_address"), "@(.*?)\\.", 1)
)
display(df_2)

# COMMAND ----------

from pyspark.sql.functions import regexp_extract, col, date_format, current_date
df_store_final = (
    df_2
    .withColumn("created_at", date_format(current_date(), "yyyy-MM-dd"))
    .withColumn("updated_at", date_format(current_date(), "yyyy-MM-dd"))
)

display(df_store_final)

# COMMAND ----------

df_dup = df_store_final.dropDuplicates(["store_id"])

# COMMAND ----------

from delta.tables import DeltaTable
 

table_name  = "medallion_architecture.sales_view.store"
 
if spark.catalog.tableExists(table_name):

    delta_table = DeltaTable.forName(spark, table_name)
 
    (

        delta_table.alias("target")

            .merge(

                df_dup.alias("source"),

                "target.store_id = source.store_id"

            )

            .whenMatchedUpdateAll()

            .whenNotMatchedInsertAll()

            .execute()

    )

else:


    df_dup.write.format("delta").mode("overwrite").saveAsTable(table_name)


# COMMAND ----------



# COMMAND ----------

df4 = spark.read.csv(
    "abfss://bronze@silva.dfs.core.windows.net/sales",
    header=True,
    inferSchema=True
)

display(df4)

import re

def rename_columns_to_snake_case(dfsales):
    def camel_to_snake(colname):
        s1 = re.sub('(.)([A-Z][a-z]+)', r'\1_\2', colname)
        colname = re.sub('([a-z0-9])([A-Z])', r'\1_\2', s1)
        return colname.lower().replace(" ", "_")
    return dfsales.toDF(*[camel_to_snake(c) for c in dfsales.columns])

def clean_column_names(dfsales):
    new_cols = []
    for c in dfsales.columns:
        clean = re.sub(r"[ ,;{}()\n\t=]", "_", c)
        clean = re.sub("_+", "_", clean)
        new_cols.append(clean.lower())
    return dfsales.toDF(*new_cols)

df_sales = clean_column_names(rename_columns_to_snake_case(df4))
display(df_sales)

# COMMAND ----------

#Convert all date columns → yyyy-MM-dd
 
#First detect date columns dynamically
date_columns = [c for c in df_sales.columns if "date" in c]
print("Date columns:", date_columns)
 
#Apply on that columns
from pyspark.sql.functions import col, to_timestamp, to_date
 
for dc in date_columns:
    df_datechange = (
        df_sales
        .withColumn(dc + "_ts", to_timestamp(col(dc), "dd-MM-yyyy HH:mm"))
        .withColumn(dc + "_date", to_date(col(dc + "_ts")))
        .drop(dc)        
        .drop(dc + "_ts")  
        .withColumnRenamed(dc + "_date", dc)
    )
 
display(df_datechange)

# COMMAND ----------

df_redup = df_datechange.dropDuplicates(["order_id", "product_id", "customer_id"])

# COMMAND ----------

from delta.tables import DeltaTable
 
table_name = "medallion_architecture.sales_view.sales"
 
if spark.catalog.tableExists(table_name):
    delta_table = DeltaTable.forName(spark, table_name)
 
    (
        delta_table.alias("target")
        .merge(
            df_redup.alias("source"),
            "target.order_id = source.order_id AND "
            "target.product_id = source.product_id AND "
            "target.customer_id = source.customer_id"
        )
        .whenMatchedUpdateAll()
        .whenNotMatchedInsertAll()
        .execute()
    )
else:
    df_redup.write.format("delta").mode("overwrite").saveAsTable(table_name)
 
print("Sales data successfully upserted into Silver layer Catalog.")

# COMMAND ----------

df_load_customer = spark.table("silver.sales_view.customer")
df_load_customer.write.mode("overwrite").format("delta").option("path", "abfss://silver@silva.dfs.core.windows.net/customer")saveAsTable("silver.sales_view.customer")