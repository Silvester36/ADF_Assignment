# Databricks notebook source
from pyspark.sql.functions import *
from delta.tables import DeltaTable
import re

def rename_columns_to_snake_case(df):
    def camel_to_snake(colname):
        s1 = re.sub('(.)([A-Z][a-z]+)', r'\1_\2', colname)
        colname = re.sub('([a-z0-9])([A-Z])', r'\1_\2', s1)
        return colname.lower().replace(" ", "_").strip()
    return df.toDF(*[camel_to_snake(c) for c in df.columns])

def clean_column_names(df):
    new_cols = []
    for c in df.columns:
        clean = re.sub(r"[ ,;{}()\n\t=]", "_", c)
        clean = re.sub("_+", "_", clean)
        new_cols.append(clean.lower())
    return df.toDF(*new_cols)


# COMMAND ----------

product_df = spark.read.format("delta").table("silver.sales_view.product")
store_df   = spark.read.format("delta").table("silver.sales_view.store")
sales_df   = spark.read.format("delta").table("silver.sales_view.sales")


# COMMAND ----------

product_df = clean_column_names(rename_columns_to_snake_case(product_df))
store_df   = clean_column_names(rename_columns_to_snake_case(store_df))
sales_df   = clean_column_names(rename_columns_to_snake_case(sales_df))


# COMMAND ----------

df_gold = (
    sales_df
    .join(product_df, "product_id", "inner")
    .join(store_df, "store_id", "inner")
    .select(
        "order_id",
        "order_date",
        "customer_id",
        "product_id",
        "store_id",
        "sales",
        "profit",
        "quantity",
        "region",
        "city",
        "segment",
        "ship_date",
        "ship_mode",

        # Product data
        "product_name",
        "category",
        "price",
        "supplier_id",

        # Store data
        "store_name",
        "location",
        "manager_name"
    )
)


# COMMAND ----------

gold_product_sales = df_gold_base.groupBy("product_id", "product_name") \
    .agg(
        sum("sales").alias("total_sales"),
        sum("profit").alias("total_profit"),
        avg("sales").alias("avg_sales"),
        count("order_id").alias("total_orders")
    )

gold_store_sales = df_gold_base.groupBy("store_id", "store_name") \
    .agg(
        sum("sales").alias("total_sales"),
        sum("profit").alias("total_profit"),
        count("order_id").alias("total_orders"),
        avg("sales").alias("avg_sales")
    )

gold_store_sales = df_gold_base.groupBy("store_id", "store_name") \
    .agg(
        sum("sales").alias("total_sales"),
        sum("profit").alias("total_profit"),
        count("order_id").alias("total_orders"),
        avg("sales").alias("avg_sales")
    )

gold_customer_clv = df_gold_base.groupBy("customer_id") \
    .agg(
        sum("sales").alias("total_spent"),
        sum("profit").alias("total_profit"),
        count("order_id").alias("orders_count")
    )



# COMMAND ----------

table_name = "gold.sales_view.StoreProductSalesAnalysis"
 
df_gold = df_gold_base.write.format("delta").mode("overwrite").saveAsTable(table_name)
display(df_gold)

# COMMAND ----------

df_load_customer = spark.table("silver.sales_view.customer")
df_load_customer.write.mode("overwrite").format("delta").save("abfss://silver@silva.dfs.core.windows.net/customer")